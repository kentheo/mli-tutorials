{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CO416 - Machine Learning for Imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2 - Scikit Learn & Image Classification\n",
    "\n",
    "\n",
    "## Regularisation \n",
    "\n",
    "Machine Learning models may overfit to your training data (as discussed in Lecture 2 from slide 35). To avoid overfitting we add to our loss function a regularisation term controlled by a weighting $\\lambda$. \n",
    "\n",
    "In the lectures you learned about two elementary regularisers, the L1 and L2 regularisation also known as Lasso and Ridge penalty, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct \n",
    "import gzip\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.neural_network\n",
    "import sklearn.decomposition\n",
    "import sklearn.pipeline# adjust settings to plot nice figures inline\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = '/vol/lab/course/416/data/mnist'\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "# train data\n",
    "train_set = dset.MNIST(root=data_dir, train=True, download=False)\n",
    "train_x_mnist = np.array(train_set.train_data)\n",
    "train_y_mnist = np.array(train_set.train_labels)\n",
    "\n",
    "# test data\n",
    "test_set = dset.MNIST(root=data_dir, train=False, download=False)\n",
    "test_x_mnist = np.array(test_set.test_data)\n",
    "test_y_mnist = np.array(test_set.test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Extract sample digits \n",
    "############################################################################\n",
    "\n",
    "def sample_data_digits(data, labels, labels_to_select):\n",
    "    # convert input 3d arrays to 2d arrays\n",
    "    \n",
    "    nsamples, nx, ny = data.shape\n",
    "    \n",
    "    data_vec = np.reshape(data,(nsamples,nx*ny))\n",
    "   \n",
    "    selected_indexes = np.isin(labels, labels_to_select)\n",
    "    selected_data = data_vec[selected_indexes]\n",
    "    selected_labels = labels[selected_indexes]\n",
    "    \n",
    "    \n",
    "    # Convert images from gray to binary by thresholding intensity values\n",
    "    selected_data = 1.0 * (selected_data >= 128)\n",
    "\n",
    "    # convert labels to binary: digit_1=False, digit_2=True\n",
    "    selected_labels = selected_labels==labels_to_select[1]\n",
    "    # shuffle data\n",
    "    shuffle_index = np.random.permutation(len(selected_labels))\n",
    "    selected_data, selected_labels = selected_data[shuffle_index], selected_labels[shuffle_index]\n",
    "     \n",
    "    return selected_data, selected_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold.t_sne import TSNE\n",
    "from sklearn.neighbors.classification import KNeighborsClassifier\n",
    "def plot_decision_boundary(model,X,y):\n",
    "    Y_pred=model.predict(X)\n",
    "    X_Train_embedded = TSNE(n_components=2).fit_transform(X)\n",
    "    # create meshgrid\n",
    "    resolution = 100 # 100x100 background pixels\n",
    "    X2d_xmin, X2d_xmax = np.min(X_Train_embedded[:,0]), np.max(X_Train_embedded[:,0])\n",
    "    X2d_ymin, X2d_ymax = np.min(X_Train_embedded[:,1]), np.max(X_Train_embedded[:,1])\n",
    "    xx, yy = np.meshgrid(np.linspace(X2d_xmin, X2d_xmax, resolution), np.linspace(X2d_ymin, X2d_ymax, resolution))\n",
    "\n",
    "    # approximate Voronoi tesselation on resolution x resolution grid using 1-NN\n",
    "    background_model = KNeighborsClassifier(n_neighbors=1).fit(X_Train_embedded, Y_pred) \n",
    "    voronoiBackground = background_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    voronoiBackground = voronoiBackground.reshape((resolution, resolution))\n",
    "\n",
    "    #plot\n",
    "    plt.contourf(xx, yy, voronoiBackground)\n",
    "    plt.scatter(X_Train_embedded[:,0], X_Train_embedded[:,1], c=y)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Regularisers \n",
    "\n",
    "We will first use a toy example to better understand the concept of regularisation. The dataset to be used here is the *'make moons'* one which can be directly used from sklearn\n",
    "\n",
    "**TASK**\n",
    "- Train a SGD classifier with `loss='log'` to implement a logistic regression. \n",
    "- Use all regularisations and evaluate the performance of your model.\n",
    "- Does your model overfit less or more after reguralization ? \n",
    "\n",
    "As you saw regularization does not affect the performance when the feature space is small. In order then to augment our feature space we make polynomial combinations of the existing features.\n",
    "\n",
    "- Using sklearns sklearn.preprocessing.PolynomialFeatures with different order polynomials we can create polynomial combinations of our features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X, Y = make_moons(1000, noise=0.1)\n",
    "#####################################################################\n",
    "# Perform a train test split , use 20% of your dataset for testing.\n",
    "#####################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "#####################################################################\n",
    "# Perform a polynomial feature transform  use degree 10 to start and then explore different polynomial degrees\n",
    "#####################################################################\n",
    "pol_transform = sklearn.preprocessing.PolynomialFeatures(degree=10)\n",
    "x_train_new = pol_transform.fit_transform(X_train)\n",
    "x_test_new = pol_transform.fit_transform(X_test)\n",
    "a=x_test_new\n",
    "for pen in ['None', 'L1', 'L2']:\n",
    "    np.random.seed(42)\n",
    "\n",
    "    print(pen)\n",
    "    #####################################################################\n",
    "    # Set up a SGD Classifier for various penalties , train and evaluate it.\n",
    "    #####################################################################\n",
    "    model = SGDClassifier(loss='log', penalty=pen, alpha=0.002, tol=None)\n",
    "    model.fit(X_train, y_train)\n",
    "    scores = model.score(X_test, y_test)\n",
    "    print('normal {}'.format(scores))\n",
    "    # Predict the function value for the whole test set\n",
    "    #     plot_decision_boundary(model,X_test,y_test)\n",
    "\n",
    "    #####################################################################\n",
    "    # Using the polynomial features, train and evaluate your classifer.\n",
    "    #####################################################################\n",
    "    model.fit(x_train_new, y_train)\n",
    "    scores = model.score(x_test_new, y_test)\n",
    "\n",
    "    print('polynomial {}'.format(scores))\n",
    "    #####################################################################\n",
    "    # Plot the decision boundary \n",
    "    #####################################################################\n",
    "    plot_decision_boundary(model, x_test_new, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving Deeper \n",
    "\n",
    "Now we will move back to the MNIST dataset. Extract from the dataset digits 0 and 8.\n",
    "\n",
    "**TASK**\n",
    "\n",
    "Below you will find the model solution for the logistic regression classifier as defined in Tutorial 1. \n",
    "- Modify this class to take one more argument that is the penalty and apply the penalty function to the loss. \n",
    "- Implement L1 and L2 regularisations not the elastic net.\n",
    "- Use your model and comment on how regularisation alters the performance \n",
    "\n",
    "See the lecture notes at Lecture 2 slide 39-41 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# Extract ones and eights digits from both training and testing data \n",
    "############################################################################\n",
    "labels_to_select = [0,8]\n",
    "selected_train_data, selected_train_labels = sample_data_digits(train_x_mnist,train_y_mnist,labels_to_select)\n",
    "selected_test_data, selected_test_labels = sample_data_digits(test_x_mnist,test_y_mnist,labels_to_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "class LogisticRegression(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, lr=0.05, num_iter=1000, add_bias=True, verbose=True,lambda_val=0.5,penalty='L1'):\n",
    "        self.lr = lr\n",
    "        self.lambda_val = lambda_val\n",
    "        self.verbose = verbose\n",
    "        self.num_iter = num_iter\n",
    "        self.add_bias = add_bias\n",
    "        self.penalty = penalty\n",
    "        \n",
    "    \n",
    "    def __add_bias(self, X):\n",
    "        bias = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((bias, X), axis=1)\n",
    "    \n",
    "\n",
    "    ############################################################################\n",
    "    #  compute the loss + ADD YOUR PENALTY HERE\n",
    "    ############################################################################\n",
    "    def __loss(self, h, y):\n",
    "        ''' computes loss values '''\n",
    "        y = np.array(y,dtype=float)\n",
    "        if self.penalty=='None':\n",
    "            reg=0\n",
    "        elif self.penalty=='L1':\n",
    "            reg=self.lambda_val * np.sum(np.linalg.norm(self.theta))\n",
    "        elif self.penalty=='L2':\n",
    "            reg=self.lambda_val * np.sum(self.theta**2)/2\n",
    "            \n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)+ reg ).mean()\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        ''' \n",
    "        Optimise our model using gradient descent\n",
    "        Arguments:\n",
    "            X input features\n",
    "            y labels from training data\n",
    "            \n",
    "        '''\n",
    "        if self.add_bias:\n",
    "            X = self.__add_bias(X)\n",
    "        \n",
    "        \n",
    "        ############################################################################\n",
    "        #initialise weights randomly with normal distribution N(0,1)\n",
    "        ############################################################################\n",
    "        self.theta = np.random.normal(0.0,0.01,X.shape[1])\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            ############################################################################\n",
    "            #  forward propagation\n",
    "            ############################################################################\n",
    "            z = X.dot(self.theta)\n",
    "            h = 1.0 / (1.0 + np.exp(-z))\n",
    "            ############################################################################\n",
    "            #  backward propagation + ADD YOUR PENALTY HERE\n",
    "            ############################################################################\n",
    "            # update parameters\n",
    "            if self.penalty=='None':\n",
    "                self.theta -= self.lr * ( np.dot(X.T, (h - y))) / y.size\n",
    "\n",
    "            elif self.penalty=='L1':\n",
    "                self.theta -= self.lr * ( np.dot(X.T, (h - y))+self.lambda_val*np.sign(self.theta)) / y.size\n",
    "\n",
    "            elif self.penalty=='L2':\n",
    "                self.theta -= self.lr * ( np.dot(X.T, (h - y))+self.lambda_val*self.theta) / y.size\n",
    "\n",
    "       \n",
    "            ############################################################################\n",
    "            # print loss\n",
    "            ############################################################################\n",
    "            if(self.verbose == True and i % 50 == 0):\n",
    "                h = 1.0 / (1.0 + np.exp(-X.dot(self.theta)))\n",
    "                print('loss: {} \\t'.format(self.__loss(h, y)))\n",
    "    \n",
    "    def predict_probs(self,X):\n",
    "        ''' returns output probabilities\n",
    "        '''\n",
    "        ############################################################################\n",
    "        # forward propagation\n",
    "        ############################################################################\n",
    "        if self.add_bias:\n",
    "            X = self.__add_bias(X)\n",
    "        z = X.dot(self.theta)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        ''' returns output classes\n",
    "        '''\n",
    "        return self.predict_probs(X) >= threshold\n",
    "    \n",
    "    def score(self, X,Y):\n",
    "        '''\n",
    "            Returns accuracy of model\n",
    "        '''\n",
    "        preds = self.predict(X)\n",
    "        accuracy = (preds == Y).mean()\n",
    "        return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying Regressor with regularisation\n",
    "\n",
    "In order to see the full effect of the regularisation we look into a scenario where the classifier clearly overfits. We will only use a small portion of the MNIST dataset, with 75 data points to begin with and then experiment with other values.\n",
    "\n",
    "**TASK**\n",
    "\n",
    "- Train your logistic regressor with 75 training datapoints, learning rate=1e-3 and 10,000 iteration and all possible penalties\n",
    "- Evaluate the performance of your model, using model.score(X,Y)\n",
    "- Use the weight plotting function from Tutorial 1 and comment on how reguralization affects them (Hint see slide 41 from lecture 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# Q: train our model using raw pixels , train for all penalties and for various lambda values\n",
    "#########################################################################\n",
    "for pen in ['None','L1','L2']:\n",
    "    for lam_val in [0.7]:\n",
    "        model = LogisticRegression(lr=1e-3, num_iter=10000,penalty=pen,lambda_val=lam_val,verbose=False)\n",
    "        model.fit(selected_train_data[0:75,:], selected_train_labels[0:75])\n",
    "\n",
    "        #########################################################################\n",
    "        # Q: Evaluate the trained model - compute train and test accuracies\n",
    "        # You should get accuracies around 98-99%\n",
    "        #########################################################################\n",
    "        logistic_train_error=model.score(selected_train_data[0:75,:],selected_train_labels[0:75])\n",
    "        print('{} with lambda {} training: {}'.format(pen,lam_val,logistic_train_error))\n",
    "\n",
    "\n",
    "        \n",
    "        test_preds = model.predict(selected_test_data)\n",
    "        logistic_test_error = (test_preds == selected_test_labels).mean()\n",
    "        print('{} with lambda {}: {}'.format(pen,lam_val,logistic_test_error))\n",
    "\n",
    "        #########################################################################\n",
    "        # Q: draw trained model params (weights) as an image of size (28x28)\n",
    "        #########################################################################\n",
    "        plt.imshow(model.theta[:-1].reshape(28,28))\n",
    "        plt.colorbar()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularisation using sklearn\n",
    "Now that we know how to implement logistic regression with L1 and L2 regularisation on numpy lets use an industry standard version - sklearn \n",
    "\n",
    "**TASK**\n",
    "\n",
    "Use sklearns SGD classifier with the `loss='log'` to perform logistic regression, you can change the penalty function by changing the `penalty` argument \n",
    "- Use all 10 classes\n",
    "- Do you get different performance ? \n",
    "- Why is there a difference ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "############################################################################\n",
    "# reshape to (N,728) this is number of samples N and the nuber of features 28x28=728\n",
    "x_train=np.reshape(train_x_mnist,(train_x_mnist.shape[0],train_x_mnist.shape[1]*train_x_mnist.shape[2]))\n",
    "x_test=np.reshape(test_x_mnist,(test_x_mnist.shape[0],test_x_mnist.shape[1]*test_x_mnist.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "############################################################################\n",
    "# No Regularisation \n",
    "############################################################################\n",
    "clf= SGDClassifier(loss='log',penalty='none',learning_rate='constant',eta0=1e-3,max_iter=10)\n",
    "clf.fit(x_train, train_y_mnist)\n",
    "print ('no regularisation = {}'.format(clf.score(x_test,test_y_mnist)))\n",
    "############################################################################\n",
    "# L1 Reguralization \n",
    "############################################################################\n",
    "clf= SGDClassifier(loss='log',penalty='l1',alpha=0.5,learning_rate='constant',eta0=1e-3,max_iter=10)\n",
    "clf.fit(x_train, train_y_mnist)\n",
    "print ('L1 regularisation = {}'.format(clf.score(x_test,test_y_mnist)))\n",
    "############################################################################\n",
    "# L2 Reguralization \n",
    "############################################################################\n",
    "clf= SGDClassifier(loss='log',penalty='l2',alpha=0.5,learning_rate='constant',eta0=1e-3,max_iter=10)\n",
    "clf.fit(x_train, train_y_mnist)\n",
    "print ('L2 regularisation = {}'.format(clf.score(x_test,test_y_mnist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation \n",
    "\n",
    "As you have seen so far the performance of your models heavily depend on the values of the hyperparameters you choose. As we do not know a priori how these would affect the test accuracy we reserve a portion of our training data to use as a validation set. Using the test set is forbidden as this constitutes data-snooping and it makes our experiments null, and we would find unrealistic performance in the real world. \n",
    "\n",
    "**TASK**\n",
    "- Using sklearns SGD classifier with log loss alter the validation portion.\n",
    "- Build your own validation scheme. \n",
    "   - Use the train-val-test split function and also determine a set of possible values for your hyperparameters (use different learning rates and different $\\lambda$ values for the regulariser (in sklearn the name the parameter to be $\\alpha$).\n",
    "   - Iterate though your possible values and evaluate the performance of your algorithm with the validation set \n",
    "   - Determine the best hyperparameters based on the performance on the validation set\n",
    "   - Execute a complete training and evaluate the models performance on the testing set\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "############################################################################\n",
    "# Split into train and val \n",
    "############################################################################\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_train, train_y_mnist, test_size=0.10, random_state=42)\n",
    "#Initialize best values\n",
    "best_acc=(0,0,0) \n",
    "############################################################################\n",
    "# Iterate over selected values\n",
    "############################################################################\n",
    "for lr in [1e-2,1e-3,1e-6]:\n",
    "    for al in [0.5,0.1,0.01]:\n",
    "        clf= SGDClassifier(loss='log',penalty='l1',alpha=al,learning_rate='constant',eta0=lr,max_iter=10)\n",
    "        clf.fit(X_train, y_train)\n",
    "        acc=clf.score(X_val,y_val)\n",
    "        print ('L1 lambda = {}, learning rate = {}, accuracy = {}'.format(al,lr,acc))\n",
    "        if best_acc[2]<acc:\n",
    "            best_acc=(lr,al,acc)\n",
    " # print best hyperparameters and accuracy   \n",
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus \n",
    "- Implement an Elastic Net and then compare it with sklearns implementation. Redo all the above tests with this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
